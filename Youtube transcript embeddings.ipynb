{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNB5moKMz6+dRgE47W/UfO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#YouTube Transcript Processing Pipeline with Cloud and Vector Storage Integration"],"metadata":{"id":"FvM__eOEUn04"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLFhW_C5ItxG"},"outputs":[],"source":["import os\n","import re\n","import pandas as pd\n","import numpy as np\n","import torch\n","from textblob import TextBlob\n","from gensim.models import Word2Vec\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from transformers import BertTokenizer, BertModel\n","from google.cloud import storage\n","from pinecone import Pinecone, ServerlessSpec\n","from flask import jsonify\n","\n","# Initialize Google Cloud Storage client\n","storage_client = storage.Client()\n","\n","# Set bucket and file information\n","bucket_name = 'elon-musk-chatbot-data'\n","source_blob_name = 'youtube_transcripts.txt'\n","destination_file_name = 'youtube_transcripts.txt'\n","\n","# Pinecone API key and environment\n","api_key = \"b8e351ec-57a1-46eb-96eb-31e7ef11fd77\"\n","environment = \"us-east-1\"\n","os.environ[\"PINECONE_API_KEY\"] = api_key\n","\n","# Step 1: Download file from Google Cloud Storage\n","def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n","    bucket = storage_client.bucket(bucket_name)\n","    blob = bucket.blob(source_blob_name)\n","    blob.download_to_filename(destination_file_name)\n","    print(f\"Downloaded {source_blob_name} from GCS to local file {destination_file_name}.\")\n","\n","# Step 2: Load and clean the data\n","def load_and_clean_data(file_name):\n","    with open(file_name, 'r') as file:\n","        transcript_text = file.readlines()\n","    df = pd.DataFrame(transcript_text, columns=[\"text\"])\n","\n","    # Cleaning function\n","    def clean_text(text):\n","        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n","        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n","        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n","        return text\n","\n","    df['cleaned_text'] = df['text'].apply(clean_text)\n","    return df\n","\n","# Step 3: Sentiment analysis\n","def perform_sentiment_analysis(df):\n","    def get_sentiment(text):\n","        analysis = TextBlob(text)\n","        return analysis.sentiment.polarity\n","    df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n","    return df\n","\n","# Step 4: TF-IDF feature extraction\n","def extract_tfidf_features(df):\n","    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)\n","    tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n","    feature_names = vectorizer.get_feature_names_out()\n","    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n","    return tfidf_df\n","\n","# Step 5: Word2Vec embeddings\n","def generate_word2vec_embeddings(df):\n","    tokenized_texts = df['cleaned_text'].apply(lambda x: x.split())\n","    word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n","    return word2vec_model\n","\n","# Step 6: BERT embeddings\n","def generate_bert_embeddings(df):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def get_bert_embedding(text):\n","        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n","        outputs = model(**inputs)\n","        embedding = outputs.last_hidden_state[:, 0, :].detach().numpy().flatten()\n","        return embedding\n","\n","    df['bert_embedding'] = df['cleaned_text'].apply(get_bert_embedding)\n","    return df\n","\n","# Step 7: Upload to Google Cloud Storage\n","def upload_to_gcs(bucket_name, local_file_path, destination_blob_name):\n","    bucket = storage_client.bucket(bucket_name)\n","    blob = bucket.blob(destination_blob_name)\n","    blob.upload_from_filename(local_file_path)\n","    print(f\"File {local_file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n","\n","# Step 8: Store embeddings in Pinecone\n","def store_embeddings_in_pinecone(df):\n","    pc = Pinecone(api_key=api_key)\n","\n","    index_name = \"elon-musk-embeddings-youtube\"\n","    if index_name not in pc.list_indexes().names():\n","        pc.create_index(\n","            name=index_name,\n","            dimension=768,\n","            metric='cosine',\n","            spec=ServerlessSpec(cloud='aws', region=environment)\n","        )\n","    index = pc.Index(index_name)\n","\n","    for i, row in df.iterrows():\n","        embedding = row['bert_embedding']\n","        index.upsert([(str(i), embedding)])\n","    print(\"Embeddings stored in Pinecone successfully!\")\n","\n","# Main function\n","def process_transcripts(request):\n","    download_from_gcs(bucket_name, source_blob_name, destination_file_name)\n","\n","    df = load_and_clean_data(destination_file_name)\n","    df = perform_sentiment_analysis(df)\n","\n","    # Save cleaned and sentiment data\n","    cleaned_sentiment_path = 'cleaned_sentiment_youtube.csv'\n","    df[['cleaned_text', 'sentiment']].to_csv(cleaned_sentiment_path, index=False)\n","    upload_to_gcs(bucket_name, cleaned_sentiment_path, 'cleaned_sentiment_youtube.csv')\n","\n","    # TF-IDF\n","    tfidf_df = extract_tfidf_features(df)\n","    tfidf_path = 'tfidf_features_youtube.csv'\n","    tfidf_df.to_csv(tfidf_path, index=False)\n","    upload_to_gcs(bucket_name, tfidf_path, 'tfidf_features_youtube.csv')\n","\n","    # Word2Vec\n","    word2vec_model = generate_word2vec_embeddings(df)\n","    word2vec_path = 'word2vec_vectors_youtube.txt'\n","    with open(word2vec_path, 'w') as f:\n","        for word in word2vec_model.wv.index_to_key:\n","            vector = word2vec_model.wv[word]\n","            f.write(f\"{word} {' '.join(map(str, vector))}\\n\")\n","    upload_to_gcs(bucket_name, word2vec_path, 'word2vec_vectors_youtube.txt')\n","\n","    # BERT embeddings\n","    df = generate_bert_embeddings(df)\n","    bert_path = 'bert_embeddings_youtube.csv'\n","    df[['cleaned_text', 'bert_embedding']].to_csv(bert_path, index=False)\n","    upload_to_gcs(bucket_name, bert_path, 'bert_embeddings_youtube.csv')\n","\n","    # Store in Pinecone\n","    store_embeddings_in_pinecone(df)\n","\n","    return jsonify(message=\"Processing and upload completed successfully.\"), 200\n"]},{"cell_type":"markdown","source":["\n","\n","1. **Data Download:**\n","   - Downloads a transcript file from a specified Google Cloud Storage bucket.\n","\n","2. **Data Cleaning:**\n","   - Cleans the text data to remove URLs, special characters, and extra whitespace.\n","\n","3. **Sentiment Analysis:**\n","   - Analyzes the sentiment polarity of each transcript line using TextBlob.\n","\n","4. **Feature Extraction:**\n","   - Extracts key features using:\n","     - **TF-IDF:** Generates n-gram-based term frequency-inverse document frequency features.\n","     - **Word2Vec:** Computes vector representations of words in the transcript.\n","     - **BERT:** Generates contextual embeddings for each line of text using a pretrained BERT model.\n","\n","5. **Cloud Integration:**\n","   - Saves cleaned data, sentiment data, TF-IDF features, Word2Vec vectors, and BERT embeddings to Google Cloud Storage.\n","\n","6. **Vector Storage:**\n","   - Stores BERT embeddings in a Pinecone index for efficient similarity search.\n","\n","7. **Incremental Pipeline:**\n","   - Processes and saves intermediate results incrementally to ensure robustness.\n","\n","\n"],"metadata":{"id":"swehRXqZUvzj"}},{"cell_type":"code","source":[],"metadata":{"id":"TJINlcAjU2TE"},"execution_count":null,"outputs":[]}]}