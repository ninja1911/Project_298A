{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3SSyqM8ATrF0oYGD2OXlQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This script processes twitter transcripts,YouTube transcript data to extract embeddings and perform sentiment analysis, enabling downstream use cases like text-based search or conversational AI. The code integrates various NLP techniques and utilizes cloud services for storage and deployment."],"metadata":{"id":"fATCvBF-R96D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qAd9VLJRPXT"},"outputs":[],"source":["import os\n","import re\n","import logging\n","import pandas as pd\n","import numpy as np\n","import torch\n","from textblob import TextBlob\n","from gensim.models import Word2Vec\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from transformers import BertTokenizer, BertModel\n","from google.cloud import storage\n","from pinecone import Pinecone, ServerlessSpec\n","from flask import jsonify\n","\n","# Initialize logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Initialize Google Cloud Storage client\n","storage_client = storage.Client()\n","\n","# Set bucket and file information\n","bucket_name = 'elon-musk-chatbot-data'\n","source_blob_name = 'youtube_transcripts.txt'\n","destination_file_name = 'local_youtube_transcripts.txt'\n","\n","# Pinecone API key and environment\n","api_key = \"b8e351ec-57a1-46eb-96eb-31e7ef11fd77\"\n","environment = \"us-east-1\"\n","os.environ[\"PINECONE_API_KEY\"] = api_key\n","\n","# Step 1: Download file from Google Cloud Storage\n","def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n","    logging.info(\"Starting file download from Google Cloud Storage.\")\n","    bucket = storage_client.bucket(bucket_name)\n","    blob = bucket.blob(source_blob_name)\n","    blob.download_to_filename(destination_file_name)\n","    logging.info(f\"Downloaded {source_blob_name} from GCS to local file {destination_file_name}.\")\n","\n","# Step 2: Load and clean the data\n","def load_and_clean_data(file_name):\n","    logging.info(\"Loading and cleaning data.\")\n","    with open(file_name, 'r') as file:\n","        transcript_text = file.readlines()\n","    df = pd.DataFrame(transcript_text, columns=[\"text\"])\n","\n","    # Cleaning function\n","    def clean_text(text):\n","        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n","        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n","        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n","        return text\n","\n","    df['cleaned_text'] = df['text'].apply(clean_text)\n","    logging.info(\"Data cleaning completed.\")\n","    return df\n","\n","# Step 3: Sentiment analysis\n","def perform_sentiment_analysis(df):\n","    logging.info(\"Performing sentiment analysis.\")\n","    def get_sentiment(text):\n","        analysis = TextBlob(text)\n","        return analysis.sentiment.polarity\n","    df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n","    logging.info(\"Sentiment analysis completed.\")\n","    return df\n","\n","# Step 4: TF-IDF feature extraction\n","def extract_tfidf_features(df):\n","    logging.info(\"Extracting TF-IDF features.\")\n","    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)\n","    tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n","    feature_names = vectorizer.get_feature_names_out()\n","    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n","    logging.info(\"TF-IDF extraction completed.\")\n","    return tfidf_df\n","\n","# Step 5: Word2Vec embeddings\n","def generate_word2vec_embeddings(df):\n","    logging.info(\"Generating Word2Vec embeddings.\")\n","    tokenized_texts = df['cleaned_text'].apply(lambda x: x.split())\n","    word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n","    logging.info(\"Word2Vec embeddings generated.\")\n","    return word2vec_model\n","\n","# Step 6: BERT embeddings\n","def generate_bert_embeddings(df):\n","    logging.info(\"Generating BERT embeddings.\")\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def get_bert_embedding(text):\n","        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n","        outputs = model(**inputs)\n","        embedding = outputs.last_hidden_state[:, 0, :].detach().numpy().flatten()\n","        return embedding\n","\n","    df['bert_embedding'] = df['cleaned_text'].apply(get_bert_embedding)\n","    logging.info(\"BERT embeddings generated.\")\n","    return df\n","\n","# Step 7: Upload to Google Cloud Storage\n","def upload_to_gcs(bucket_name, local_file_path, destination_blob_name):\n","    logging.info(f\"Uploading {local_file_path} to Google Cloud Storage.\")\n","    bucket = storage_client.bucket(bucket_name)\n","    blob = bucket.blob(destination_blob_name)\n","    blob.upload_from_filename(local_file_path)\n","    logging.info(f\"File {local_file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n","\n","# Step 8: Store embeddings in Pinecone\n","def store_embeddings_in_pinecone(df):\n","    logging.info(\"Storing embeddings in Pinecone.\")\n","    pc = Pinecone(api_key=api_key)\n","\n","    index_name = \"elon-musk-embeddings-youtube\"\n","    if index_name not in pc.list_indexes().names():\n","        pc.create_index(\n","            name=index_name,\n","            dimension=768,\n","            metric='cosine',\n","            spec=ServerlessSpec(cloud='aws', region=environment)\n","        )\n","    index = pc.Index(index_name)\n","\n","    for i, row in df.iterrows():\n","        embedding = row['bert_embedding']\n","        index.upsert([(str(i), embedding)])\n","    logging.info(\"Embeddings stored in Pinecone successfully.\")\n","\n","# Main function\n","def process_transcripts(request):\n","    logging.info(\"Starting transcript processing.\")\n","    download_from_gcs(bucket_name, source_blob_name, destination_file_name)\n","\n","    df = load_and_clean_data(destination_file_name)\n","    df = perform_sentiment_analysis(df)\n","\n","    # Save cleaned and sentiment data\n","    cleaned_sentiment_path = 'cleaned_sentiment_youtube.csv'\n","    df[['cleaned_text', 'sentiment']].to_csv(cleaned_sentiment_path, index=False)\n","    upload_to_gcs(bucket_name, cleaned_sentiment_path, 'cleaned_sentiment_youtube.csv')\n","\n","    # TF-IDF\n","    tfidf_df = extract_tfidf_features(df)\n","    tfidf_path = 'tfidf_features_youtube.csv'\n","    tfidf_df.to_csv(tfidf_path, index=False)\n","    upload_to_gcs(bucket_name, tfidf_path, 'tfidf_features_youtube.csv')\n","\n","    # Word2Vec\n","    word2vec_model = generate_word2vec_embeddings(df)\n","    word2vec_path = 'word2vec_vectors_youtube.txt'\n","    with open(word2vec_path, 'w') as f:\n","        for word in word2vec_model.wv.index_to_key:\n","            vector = word2vec_model.wv[word]\n","            f.write(f\"{word} {' '.join(map(str, vector))}\\n\")\n","    upload_to_gcs(bucket_name, word2vec_path, 'word2vec_vectors_youtube.txt')\n","\n","    # BERT embeddings\n","    df = generate_bert_embeddings(df)\n","    bert_path = 'bert_embeddings_youtube.csv'\n","    df[['cleaned_text', 'bert_embedding']].to_csv(bert_path, index=False)\n","    upload_to_gcs(bucket_name, bert_path, 'bert_embeddings_youtube.csv')\n","\n","    # Store in Pinecone\n","    store_embeddings_in_pinecone(df)\n","\n","    logging.info(\"Transcript processing and upload completed successfully.\")\n","    return jsonify(message=\"Processing and upload completed successfully.\"), 200"]},{"cell_type":"markdown","source":["\n","\n","1. **Cloud Integration:** Downloads transcript data from Google Cloud Storage and uploads processed files back to the cloud.\n","2. **Data Cleaning:** Cleans raw text data by removing URLs, special characters, and extra spaces.\n","3. **Sentiment Analysis:** Computes sentiment polarity for each transcript using TextBlob.\n","4. **Feature Extraction:** Generates TF-IDF features, Word2Vec embeddings, and BERT embeddings for the transcripts.\n","5. **Storage:** Saves processed data and embeddings locally and to Google Cloud Storage.\n","6. **Pinecone Indexing:** Optionally indexes BERT embeddings in Pinecone for efficient similarity search and retrieval."],"metadata":{"id":"wDF9ahIWSrMl"}},{"cell_type":"markdown","source":["This Streamlit application enables a chatbot that mimics Elon Musk's conversational style. It leverages precomputed BERT embeddings for efficient context retrieval and generates responses using an NVIDIA-based LLM.\n","\n"],"metadata":{"id":"kY_GE3fLSgLK"}},{"cell_type":"code","source":["import os\n","import ast\n","import numpy as np\n","import pandas as pd\n","import streamlit as st\n","import torch\n","from transformers import BertTokenizer, BertModel\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain_nvidia_ai_endpoints import ChatNVIDIA\n","\n","# Set up Streamlit\n","st.set_page_config(page_title=\"Musk Bot\")\n","st.header(\"ðŸ’¬ In Conversation with Elon Musk ðŸš˜ðŸš€\")\n","\n","# Load embeddings from CSV\n","embeddings_file = \"bert_embeddings.csv\"  # Path to your embeddings CSV\n","if not os.path.exists(embeddings_file):\n","    st.error(\"Embeddings file not found. Please ensure bert_embeddings_youtube.csv is present.\")\n","    st.stop()\n","\n","df = pd.read_csv(embeddings_file)\n","\n","# The 'bert_embedding' column might be stored as a string representation of a list.\n","# We'll convert each embedding back to a NumPy array.\n","def parse_embedding_string(emb_str):\n","    # ast.literal_eval safely evaluates the string to a Python list\n","    arr = ast.literal_eval(emb_str)\n","    return np.array(arr, dtype=np.float32)\n","\n","df['embedding_array'] = df['bert_embedding'].apply(parse_embedding_string)\n","\n","# Extract embeddings and texts\n","doc_embeddings = np.vstack(df['embedding_array'].values)  # shape: (num_docs, embedding_dim)\n","doc_texts = df['cleaned_text'].tolist()\n","\n","# Load the same BERT model and tokenizer used in code 1 for queries\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","def embed_text_with_bert(text):\n","    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    # We used the [CLS] token embedding: outputs.last_hidden_state[:, 0, :]\n","    embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n","    return embedding\n","\n","# Function to compute cosine similarity\n","def cosine_similarity(a, b):\n","    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n","    b_norm = b / np.linalg.norm(b)\n","    return np.dot(a_norm, b_norm)\n","\n","# Define Prompt Template (same as before)\n","prompt = PromptTemplate(\n","    input_variables=[\"document\", \"question\"],\n","    template=(\n","        \"You are Elon Musk. Respond to questions like Elon, using his characteristic style: informal, bold, with occasional slang, \"\n","        \"thought-provoking statements, and a hint of humor. Speak directly to the question, referencing details from your knowledge \"\n","        \"and the content provided below.\\n\\n\"\n","        \"Your current data includes Elon Musk's discussions, reflecting his thoughts on technology, humanity, \"\n","        \"space exploration, renewable energy, AI, and sustainability. Use these details to base your responses, adding additional \"\n","        \"knowledge when needed.\\n\\n\"\n","        \"Question: {question}\\n\\n\"\n","        \"Relevant Document Excerpts:\\n{document}\\n\\n\"\n","        \"Response in Elon Musk's tone, using slang where appropriate:\"\n","    )\n",")\n","\n","# Initialize the LLM\n","llm = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\")\n","chain = LLMChain(llm=llm, prompt=prompt)\n","\n","st.write(\"Ask Elon Musk a question:\")\n","\n","question = st.text_input(\"Your question\")\n","\n","if question:\n","    # Embed the question\n","    query_embedding = embed_text_with_bert(question)\n","\n","    # Compute similarity to all documents\n","    similarities = cosine_similarity(doc_embeddings, query_embedding)\n","\n","    # Get top-k most similar documents (e.g., top 5)\n","    top_k = 5\n","    top_indices = np.argsort(similarities)[-top_k:][::-1]\n","    relevant_docs = [doc_texts[i] for i in top_indices]\n","\n","    # Create the document context by concatenating the top documents\n","    document_context = \"\\n\".join(relevant_docs)\n","\n","    # Run the chain\n","    answer = chain.run({\"document\": document_context, \"question\": question})\n","    st.write(answer)"],"metadata":{"id":"KAg390l1RdmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","1. **Embedding Loading and Parsing:**\n","   - Loads precomputed BERT embeddings from a CSV file.\n","   - Parses embedding strings into NumPy arrays.\n","\n","2. **Similarity Search:**\n","   - Computes cosine similarity between user questions and document embeddings to retrieve the most relevant context.\n","\n","3. **Chatbot Initialization:**\n","   - Uses the NVIDIA Llama 3.2 model to generate responses.\n","   - Incorporates context from the retrieved documents to provide coherent and relevant answers.\n","\n","4. **Streamlit Integration:**\n","   - Offers an interactive user interface for asking questions and viewing Elon Musk-style responses.\n","\n","This implementation provides a scalable approach to building a personality-driven chatbot by combining BERT for context retrieval with a large language model for response generation."],"metadata":{"id":"NI_BQQMnS_a9"}},{"cell_type":"code","source":[],"metadata":{"id":"HGJmd8HeTBMA"},"execution_count":null,"outputs":[]}]}