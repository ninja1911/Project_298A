{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOVCfCb/Nr6DF5CF+0tDO2v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Elon Musk Tweet Scraper for PolitiTweet Data"],"metadata":{"id":"xRTIU8fOI_4f"}},{"cell_type":"code","source":["pip install requests beautifulsoup4 pandas\n"],"metadata":{"id":"tB3oO8BhI_i2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from datetime import datetime\n","import time\n","import os\n","\n","# Base URL for Elon Musk's tweets on PolitiTweet\n","base_url = 'https://polititweet.org/tweets?account=44196397&page={}'\n","\n","# Initialize an empty list to store tweet data\n","tweets_data = []\n","\n","# Function to parse tweet date\n","def parse_date(date_str):\n","    try:\n","        return datetime.strptime(date_str, 'Posted %B %d, %Y')\n","    except ValueError:\n","        return None\n","\n","# Function to scrape a single page of tweets\n","def scrape_page(page):\n","    url = base_url.format(page)\n","    response = requests.get(url)\n","    if response.status_code != 200:\n","        print(f\"Failed to retrieve page {page}. Status code: {response.status_code}\")\n","        return None\n","\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    tweets = soup.find_all('div', class_='columns is-mobile')\n","\n","    page_data = []\n","    for tweet in tweets:\n","        try:\n","            # Extract tweet text and remove additional tags or text\n","            tweet_text = tweet.find('p', class_='small-top-margin').text.strip()\n","            tweet_text = tweet_text.split('â€”')[0].strip()  # Remove PolitiTweet.org text\n","\n","            # Extract tweet date\n","            date_tag = tweet.find('span', class_='tag is-white is-paddingless')\n","            tweet_date_str = date_tag.text.strip() if date_tag else \"Unknown\"\n","            tweet_date = parse_date(tweet_date_str)\n","\n","            # Append the tweet data to the list\n","            page_data.append({\n","                'id': None,  # ID not available on the website\n","                'user_name': 'Elon Musk',\n","                'user_location': '',\n","                'user_description': 'Mars & Cars, Chips & Dips',\n","                'user_created': '2009-06-02 20:12:29+00:00',\n","                'user_followers': '',  # Static or unavailable on PolitiTweet\n","                'user_friends': '',\n","                'user_favourites': '',\n","                'user_verified': True,\n","                'date': tweet_date,\n","                'text': tweet_text,\n","                'hashtags': '',\n","                'source': 'PolitiTweet',\n","                'retweets': '',  # Retweets and favorites not available\n","                'favorites': '',\n","                'is_retweet': 'RT' in tweet_text\n","            })\n","\n","        except Exception as e:\n","            print(f\"Error processing tweet on page {page}: {e}\")\n","            continue\n","\n","    return page_data\n","\n","# Function to save data to CSV\n","def save_to_csv(data, filename):\n","    df = pd.DataFrame(data)\n","    df.to_csv(filename, mode='a', index=False, header=not os.path.exists(filename))\n","    print(f\"Data saved to {filename}\")\n","\n","# Main function to scrape all pages and save to CSV incrementally\n","def scrape_all_pages(start_page=1, max_pages=824):\n","    page = start_page\n","    while page <= max_pages:\n","        print(f\"Scraping page {page}...\")\n","\n","        try:\n","            page_data = scrape_page(page)\n","\n","            # If no data is returned, assume an error occurred\n","            if not page_data:\n","                print(f\"No data on page {page}. Retrying...\")\n","                time.sleep(5)  # Wait and retry\n","                continue\n","\n","            # Append current page data to main data list\n","            tweets_data.extend(page_data)\n","\n","            # Save each page incrementally to CSV\n","            save_to_csv(page_data, 'elonmusk_polititweet.csv')\n","\n","            # Move to the next page\n","            page += 1\n","            time.sleep(1)  # Pause to avoid overloading the server\n","\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Connection error on page {page}: {e}\")\n","            print(\"Retrying after a short delay...\")\n","            time.sleep(10)  # Wait 10 seconds before retrying\n","\n","    print(\"All pages scraped and data saved.\")\n","\n","# Start scraping from the first page, adjust if restarting from a specific page\n","scrape_all_pages(start_page=1, max_pages=824)\n"],"metadata":{"id":"yQfpydjiI_fG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","1. **Web Scraping:**\n","   - Sends HTTP requests to the PolitiTweet website to retrieve tweet pages.\n","   - Parses HTML using BeautifulSoup to extract tweet text and metadata such as date and source.\n","\n","2. **Data Processing:**\n","   - Extracts and formats relevant tweet details, including:\n","     - Text content\n","     - Posting date\n","     - User details (static placeholders for PolitiTweet)\n","     - Indicators for retweets and hashtags.\n","\n","3. **Error Handling and Retries:**\n","   - Includes mechanisms to handle HTTP errors, connection issues, and malformed HTML.\n","   - Retries scraping when a page fails or returns no data.\n","\n","4. **Data Storage:**\n","   - Saves extracted tweets to a CSV file incrementally.\n","   - Appends new data to the CSV after scraping each page, ensuring progress is saved.\n","\n","5. **Customization:**\n","   - Allows starting from a specific page and defining the total number of pages to scrape (`start_page` and `max_pages`)."],"metadata":{"id":"Jw31E_8KTySh"}},{"cell_type":"markdown","source":["###  Google Cloud Storage in Colab"],"metadata":{"id":"nyaRpQfhkC0Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xaAUzQxj--4"},"outputs":[],"source":["from google.colab import auth\n","auth.authenticate_user()\n"]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"l3gvIPIkr6WO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from google.cloud import storage\n","\n","# Load the CSV file (optional if you only want to upload it)\n","df = pd.read_csv(\"elonmusk_polititweet.csv\")\n","\n","# Define local file path\n","local_file_path = \"elonmusk_polititweet.csv\"\n","\n","# Set up Google Cloud Storage client\n","storage_client = storage.Client()\n","\n","# Define bucket and destination in GCS\n","bucket_name = 'elon-musk-chatbot-data'\n","destination_blob_name = 'elonmusk_polititweet.csv'\n","\n","def upload_to_gcs(bucket_name, local_file_path, destination_blob_name):\n","    bucket = storage_client.bucket(bucket_name)\n","    blob = bucket.blob(destination_blob_name)\n","    blob.upload_from_filename(local_file_path)\n","    print(f\"File {local_file_path} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n","\n","# Upload CSV to Google Cloud Storage\n","upload_to_gcs(bucket_name, local_file_path, destination_blob_name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uO_dDXVekFiZ","executionInfo":{"status":"ok","timestamp":1730850021247,"user_tz":480,"elapsed":1788,"user":{"displayName":"nivi gana","userId":"04046597253666951830"}},"outputId":"7d5fc507-54b1-4df0-f1e0-f78e7f4a8f39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File elonmusk_polititweet.csv uploaded to elonmusk_polititweet.csv in bucket elon-musk-chatbot-data.\n"]}]},{"cell_type":"markdown","source":["This script uploads a local CSV file (elonmusk_polititweet.csv) containing Elon Musk's tweets to a specified Google Cloud Storage bucket. The file can be used for further analysis or integration with other projects in the cloud."],"metadata":{"id":"ui6aB84EUG7_"}},{"cell_type":"code","source":[],"metadata":{"id":"cgASVUMemhkl"},"execution_count":null,"outputs":[]}]}